{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from cv2 import VideoWriter, VideoWriter_fourcc\n",
    "\n",
    "env = gym.make('BipedalWalker-v2')\n",
    "class Normalizer():\n",
    "    def __init__(self):\n",
    "        self.n = np.zeros(nb_inputs)\n",
    "        self.mean = np.zeros(nb_inputs)\n",
    "        self.mean_diff = np.zeros(nb_inputs)\n",
    "        self.var = np.zeros(nb_inputs)\n",
    "        \n",
    "class Hp():\n",
    "    def __init__(self):\n",
    "        self.nb_steps = 1000\n",
    "        self.episode_length = 1000\n",
    "        self.learning_rate = 0.02\n",
    "        self.nb_directions = 16\n",
    "        self.nb_best_directions = 16\n",
    "        self.noise = 0.03\n",
    "\n",
    "def record(theta, directory):\n",
    "    \n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    sum_rewards = 0\n",
    "    num_steps = 0\n",
    "    images = []\n",
    "    while not done and num_steps < hp.episode_length:\n",
    "        state = normalize(state)\n",
    "        action = theta.dot(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        images.append(env.render(mode='rgb_array'))\n",
    "        reward = max(min(reward, 1), -1)\n",
    "        sum_rewards += reward\n",
    "        num_steps +=1\n",
    "    print(sum_rewards)\n",
    "    for x in range(len(images)):\n",
    "        blue = images[x][:,:,0].copy() #0-> blue\n",
    "        #green = images[x][:,:,1].copy()  #1-> green\n",
    "        red = images[x][:,:,2].copy() #2-> red\n",
    "    \n",
    "        images[x][:,:,0] = red\n",
    "        #images[x][:,:,1] = green\n",
    "        images[x][:,:,2] = blue\n",
    "    FPS = 24\n",
    "\n",
    "    fourcc = VideoWriter_fourcc(*'MP42')\n",
    "    #./noise.avi\n",
    "    video = VideoWriter(directory, fourcc, float(FPS), (images[0].shape[1], images[0].shape[0]))\n",
    "\n",
    "    for x in range(len(images)):\n",
    "        frame = images[x]\n",
    "        video.write(frame)\n",
    "    video.release()\n",
    "    env.close()\n",
    "    \n",
    "def normalize(state):\n",
    "    normalizer.n += 1\n",
    "    last_mean = normalizer.mean.copy()\n",
    "    #Incremental mean \n",
    "    normalizer.mean += (state - normalizer.mean) / normalizer.n\n",
    "    \n",
    "    #Get variance\n",
    "    normalizer.mean_diff += (state - last_mean) * (state - normalizer.mean)\n",
    "    normalizer.var = (normalizer.mean_diff / normalizer.n).clip(min=1e-2)\n",
    "    \n",
    "    obs_mean = normalizer.mean\n",
    "    #Get standard derivation\n",
    "    obs_std = np.sqrt(normalizer.var)\n",
    "    \n",
    "    return (state - obs_mean) / obs_std\n",
    "\n",
    "def get_deltas():\n",
    "    delta = []\n",
    "    for _ in range(hp.nb_directions):\n",
    "        delta.append(np.random.randn(*weights.shape))\n",
    "    return delta\n",
    "\n",
    "def evaluate(state, delta=None, direction=None):\n",
    "    if direction is None:\n",
    "        return weights.dot(state)\n",
    "    elif direction == \"positive\":\n",
    "        return (weights + hp.noise*delta).dot(state)\n",
    "    else:\n",
    "        return (weights - hp.noise*delta).dot(state)\n",
    "\n",
    "def explore(direction=None, delta=None, render=False):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    num_plays = 0\n",
    "    sum_rewards = 0\n",
    "    while not done and num_plays < hp.episode_length:\n",
    "        state = normalize(state)\n",
    "        action = evaluate(state, delta, direction)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        #if render:\n",
    "        #    env.render()\n",
    "        reward = max(min(reward, 1), -1)\n",
    "        sum_rewards += reward\n",
    "        num_plays +=1\n",
    "    return sum_rewards\n",
    "\n",
    "def select_rewards_delta(positive_rewards, negative_rewards, deltas):\n",
    "    r_to_delete = hp.nb_directions - hp.nb_best_directions\n",
    "    top_rewards = np.maximum(positive_rewards, negative_rewards)\n",
    "    index = np.argsort(top_rewards)\n",
    "    index_to_slice = index[:r_to_delete]\n",
    "    positive_rewards = np.delete(positive_rewards, index_to_slice)\n",
    "    negative_rewards = np.delete(negative_rewards, index_to_slice)\n",
    "    for ele in sorted(index_to_slice, reverse = True):  \n",
    "        del deltas[ele]\n",
    "    rollouts = []\n",
    "    for k in range(hp.nb_best_directions):\n",
    "        rollouts.append((positive_rewards[k],negative_rewards[k], deltas[k]))\n",
    "    return rollouts\n",
    "\n",
    "def train(weights):\n",
    "    for step in range(hp.nb_steps+1):\n",
    "        deltas = get_deltas()\n",
    "        positive_rewards = [0] * hp.nb_directions\n",
    "        negative_rewards = [0] * hp.nb_directions\n",
    "        \n",
    "        for k in range(hp.nb_directions):\n",
    "            positive_rewards[k] = explore(direction =\"positive\",delta=deltas[k])\n",
    "        for k in range(hp.nb_directions):\n",
    "            negative_rewards[k] = explore(direction =\"negative\",delta=deltas[k])\n",
    "            \n",
    "        all_rewards = np.array(positive_rewards + negative_rewards)\n",
    "        std_r = all_rewards.std()\n",
    "        \n",
    "        \n",
    "        rollouts = select_rewards_delta(positive_rewards, negative_rewards, deltas)\n",
    "        change = np.zeros(weights.shape)\n",
    "        for r_pos, r_neg, d in rollouts:\n",
    "            change += (r_pos - r_neg) * d\n",
    "        weights += hp.learning_rate / (hp.nb_best_directions * std_r) * change\n",
    "        \n",
    "        if step % 5 == 0:\n",
    "            reward_evaluation = explore()\n",
    "            print('Step: ', step, 'Reward: ', reward_evaluation)\n",
    "            if reward_evaluation > 300:\n",
    "                record(weights, str(\"./videos/\"+str(step)+\".avi\"))\n",
    "                return weights\n",
    "            if step % 100 == 0:\n",
    "                record(weights, str(\"./videos\"+str(step)+\".avi\"))\n",
    "    return weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0 Reward:  3.396299866428019\n",
      "3.293887263511076\n",
      "Step:  5 Reward:  6.184347310942232\n",
      "Step:  10 Reward:  4.3099152809357495\n",
      "Step:  15 Reward:  3.2708267312283485\n",
      "Step:  20 Reward:  4.376835916242026\n",
      "Step:  25 Reward:  5.3402004016314955\n",
      "Step:  30 Reward:  4.683849774803049\n",
      "Step:  35 Reward:  -12.157536014746771\n",
      "Step:  40 Reward:  5.696218181535611\n",
      "Step:  45 Reward:  3.6902223509357306\n",
      "Step:  50 Reward:  6.54139221284329\n",
      "Step:  55 Reward:  9.414753497953038\n",
      "Step:  60 Reward:  4.698239368437971\n",
      "Step:  65 Reward:  5.806239317061675\n",
      "Step:  70 Reward:  10.490950807273032\n",
      "Step:  75 Reward:  10.318066050365724\n",
      "Step:  80 Reward:  106.33421207353017\n",
      "Step:  85 Reward:  8.86372408274554\n",
      "Step:  90 Reward:  78.72353061390903\n",
      "Step:  95 Reward:  34.27130771851187\n",
      "Step:  100 Reward:  92.79043255585516\n",
      "121.86234917377433\n",
      "Step:  105 Reward:  51.61904278104626\n",
      "Step:  110 Reward:  54.833878020978105\n",
      "Step:  115 Reward:  142.8315329165361\n",
      "Step:  120 Reward:  136.62695802505746\n",
      "Step:  125 Reward:  148.8514923022779\n",
      "Step:  130 Reward:  114.37880301267381\n",
      "Step:  135 Reward:  21.003918770392954\n",
      "Step:  140 Reward:  38.91928840406074\n",
      "Step:  145 Reward:  156.2057740847838\n",
      "Step:  150 Reward:  153.32114633099508\n",
      "Step:  155 Reward:  144.92000417835595\n",
      "Step:  160 Reward:  144.4980557307253\n",
      "Step:  165 Reward:  148.46045476176855\n",
      "Step:  170 Reward:  144.91275885665866\n",
      "Step:  175 Reward:  128.33275711207884\n",
      "Step:  180 Reward:  136.60873609414833\n",
      "Step:  185 Reward:  138.63645079340094\n",
      "Step:  190 Reward:  118.96899358459669\n",
      "Step:  195 Reward:  134.29579549659942\n",
      "Step:  200 Reward:  151.43745167243583\n",
      "65.79607077776498\n",
      "Step:  205 Reward:  152.05897961265205\n",
      "Step:  210 Reward:  146.90163404456123\n",
      "Step:  215 Reward:  55.168848875549614\n",
      "Step:  220 Reward:  136.52322458264945\n",
      "Step:  225 Reward:  63.786639213797216\n",
      "Step:  230 Reward:  139.6827194385583\n",
      "Step:  235 Reward:  71.99665149744095\n",
      "Step:  240 Reward:  147.2656242324245\n",
      "Step:  245 Reward:  160.20504675207084\n",
      "Step:  250 Reward:  150.96657250694668\n",
      "Step:  255 Reward:  150.03450000042685\n",
      "Step:  260 Reward:  138.90961455591975\n",
      "Step:  265 Reward:  152.00811160449342\n",
      "Step:  270 Reward:  155.57742811740536\n",
      "Step:  275 Reward:  146.27923334359247\n",
      "Step:  280 Reward:  141.63534438424202\n",
      "Step:  285 Reward:  162.521944745363\n",
      "Step:  290 Reward:  159.93559573776864\n",
      "Step:  295 Reward:  120.4025110926272\n",
      "Step:  300 Reward:  91.9803446856319\n",
      "161.1841801149231\n",
      "Step:  305 Reward:  141.77910904640456\n",
      "Step:  310 Reward:  146.59885421037205\n",
      "Step:  315 Reward:  145.29104075313577\n",
      "Step:  320 Reward:  159.5232582900726\n",
      "Step:  325 Reward:  153.70925674347532\n",
      "Step:  330 Reward:  159.8951445880307\n",
      "Step:  335 Reward:  159.1536466321317\n",
      "Step:  340 Reward:  164.5529024339831\n",
      "Step:  345 Reward:  164.09184258260387\n",
      "Step:  350 Reward:  127.4221642025671\n",
      "Step:  355 Reward:  150.84703185835727\n",
      "Step:  360 Reward:  176.30552596890604\n",
      "Step:  365 Reward:  167.80588336407126\n",
      "Step:  370 Reward:  159.1958910260441\n",
      "Step:  375 Reward:  71.90030211342304\n",
      "Step:  380 Reward:  170.21556335572532\n",
      "Step:  385 Reward:  160.69613837537082\n",
      "Step:  390 Reward:  165.6700229171684\n",
      "Step:  395 Reward:  157.57987413598048\n",
      "Step:  400 Reward:  163.98110681902853\n",
      "158.67155464900011\n",
      "Step:  405 Reward:  151.51392628530579\n",
      "Step:  410 Reward:  158.67468913141897\n",
      "Step:  415 Reward:  166.77376708424586\n",
      "Step:  420 Reward:  177.6838506045875\n",
      "Step:  425 Reward:  168.81368721340849\n",
      "Step:  430 Reward:  170.40368757550607\n",
      "Step:  435 Reward:  78.8913271590941\n",
      "Step:  440 Reward:  39.71206151376914\n",
      "Step:  445 Reward:  183.56283053802483\n",
      "Step:  450 Reward:  172.96869193876566\n",
      "Step:  455 Reward:  174.22167961449762\n",
      "Step:  460 Reward:  184.90316678434544\n",
      "Step:  465 Reward:  9.788714510450715\n",
      "Step:  470 Reward:  158.0838706223433\n",
      "Step:  475 Reward:  167.0745099688101\n",
      "Step:  480 Reward:  162.9682999401245\n",
      "Step:  485 Reward:  173.4135009277662\n",
      "Step:  490 Reward:  166.41165255513917\n",
      "Step:  495 Reward:  179.93183936946497\n",
      "Step:  500 Reward:  175.645272686664\n",
      "171.12493489728308\n",
      "Step:  505 Reward:  184.5604868055405\n",
      "Step:  510 Reward:  169.77858407367424\n",
      "Step:  515 Reward:  178.03120069929787\n",
      "Step:  520 Reward:  180.9742819827627\n",
      "Step:  525 Reward:  149.75687271996037\n",
      "Step:  530 Reward:  187.13516710959698\n",
      "Step:  535 Reward:  188.71751978044304\n",
      "Step:  540 Reward:  186.8347960604497\n",
      "Step:  545 Reward:  188.55591815172772\n",
      "Step:  550 Reward:  175.2669632645658\n",
      "Step:  555 Reward:  185.09685388425882\n",
      "Step:  560 Reward:  186.54075405940432\n",
      "Step:  565 Reward:  176.02904106154185\n",
      "Step:  570 Reward:  183.8867606426781\n",
      "Step:  575 Reward:  186.82321191622427\n",
      "Step:  580 Reward:  198.97350374878127\n",
      "Step:  585 Reward:  177.80208601350435\n",
      "Step:  590 Reward:  187.4379199208057\n",
      "Step:  595 Reward:  191.08469184109575\n",
      "Step:  600 Reward:  195.67883484675332\n",
      "189.3513619943323\n",
      "Step:  605 Reward:  197.63432876249428\n",
      "Step:  610 Reward:  197.14821766551069\n",
      "Step:  615 Reward:  178.37431909006233\n",
      "Step:  620 Reward:  169.02635936315016\n",
      "Step:  625 Reward:  195.33681192413346\n",
      "Step:  630 Reward:  202.05400514187014\n",
      "Step:  635 Reward:  176.53791541414856\n",
      "Step:  640 Reward:  198.45887906877326\n",
      "Step:  645 Reward:  195.47952246246223\n",
      "Step:  650 Reward:  185.9806985196988\n",
      "Step:  655 Reward:  188.59298245034586\n",
      "Step:  660 Reward:  176.38087045081602\n",
      "Step:  665 Reward:  174.13779827420015\n",
      "Step:  670 Reward:  185.3285658020042\n",
      "Step:  675 Reward:  188.00199240073152\n",
      "Step:  680 Reward:  188.34549546676053\n",
      "Step:  685 Reward:  183.75839416003006\n",
      "Step:  690 Reward:  179.30615195524635\n",
      "Step:  695 Reward:  93.97188637140034\n",
      "Step:  700 Reward:  182.60067835556538\n",
      "188.03684378771717\n",
      "Step:  705 Reward:  184.73103129914102\n",
      "Step:  710 Reward:  190.27454880451435\n",
      "Step:  715 Reward:  186.11261589951386\n",
      "Step:  720 Reward:  201.92637313697742\n",
      "Step:  725 Reward:  116.04758170339049\n",
      "Step:  730 Reward:  41.60909784776434\n",
      "Step:  735 Reward:  212.15531532301245\n",
      "Step:  740 Reward:  194.27817653593252\n",
      "Step:  745 Reward:  77.21084345622799\n",
      "Step:  750 Reward:  196.17977978184504\n",
      "Step:  755 Reward:  200.88467723360895\n",
      "Step:  760 Reward:  197.4175068911167\n",
      "Step:  765 Reward:  186.3408309611021\n",
      "Step:  770 Reward:  200.90796929661457\n",
      "Step:  775 Reward:  195.35313184124854\n",
      "Step:  780 Reward:  194.4610326397178\n",
      "Step:  785 Reward:  186.47935947846156\n",
      "Step:  790 Reward:  190.78412258799858\n",
      "Step:  795 Reward:  172.41906500281613\n",
      "Step:  800 Reward:  194.79676752364207\n",
      "203.3533487508497\n",
      "Step:  805 Reward:  207.90082993732165\n",
      "Step:  810 Reward:  199.11339952057617\n",
      "Step:  815 Reward:  186.81508895285342\n",
      "Step:  820 Reward:  184.17249766773207\n",
      "Step:  825 Reward:  203.90177382898779\n",
      "Step:  830 Reward:  200.81787050467008\n",
      "Step:  835 Reward:  197.7634588835558\n",
      "Step:  840 Reward:  195.22733411927626\n",
      "Step:  845 Reward:  188.1474866201701\n",
      "Step:  850 Reward:  203.20752082218917\n",
      "Step:  855 Reward:  197.13853026853602\n",
      "Step:  860 Reward:  81.94585997624559\n",
      "Step:  865 Reward:  213.60662279252006\n",
      "Step:  870 Reward:  204.60485709582724\n",
      "Step:  875 Reward:  208.11567531749773\n",
      "Step:  880 Reward:  211.15691031677454\n",
      "Step:  885 Reward:  216.43812571240278\n",
      "Step:  890 Reward:  203.66051021126447\n",
      "Step:  895 Reward:  210.51625251094063\n",
      "Step:  900 Reward:  210.02093557381158\n",
      "198.63708649503363\n",
      "Step:  905 Reward:  206.64430609488534\n",
      "Step:  910 Reward:  210.83706059498942\n",
      "Step:  915 Reward:  72.5311159279861\n",
      "Step:  920 Reward:  212.58047819436848\n",
      "Step:  925 Reward:  225.72508819648223\n",
      "Step:  930 Reward:  216.69295106176247\n",
      "Step:  935 Reward:  205.30604382837052\n",
      "Step:  940 Reward:  229.73252848931756\n",
      "Step:  945 Reward:  213.4473299746903\n",
      "Step:  950 Reward:  66.05785467506453\n",
      "Step:  955 Reward:  197.9096443640343\n",
      "Step:  960 Reward:  219.50417514337423\n",
      "Step:  965 Reward:  221.60545840802936\n",
      "Step:  970 Reward:  210.18213297845844\n",
      "Step:  975 Reward:  206.44155273685533\n",
      "Step:  980 Reward:  213.69097864359384\n",
      "Step:  985 Reward:  201.54440503308624\n",
      "Step:  990 Reward:  209.4841980357965\n",
      "Step:  995 Reward:  217.12134503078886\n",
      "Step:  1000 Reward:  219.7253907287421\n",
      "218.30908824872955\n"
     ]
    }
   ],
   "source": [
    "nb_inputs = env.observation_space.shape[0]\n",
    "nb_outputs = env.action_space.shape[0]\n",
    "weights = np.zeros((nb_outputs, nb_inputs))\n",
    "hp = Hp()\n",
    "normalizer = Normalizer()\n",
    "\n",
    "weights = train(weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
